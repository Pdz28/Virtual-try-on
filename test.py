# coding=utf-8
import torch
import torch.nn as nn
import torch.nn.functional as F

import argparse
import os
import time
import glob
import shutil
import cv2
import numpy as np
from cp_dataset import CPDataset, CPDataLoader
from networks import GMM, UnetGenerator, load_checkpoint
from gen_data import process_and_prepare_data

from tensorboardX import SummaryWriter
from visualization import board_add_image, board_add_images, save_images


def upscale_with_opencv(image_path, output_path="upscaled_opencv.jpg"):
    """Upscale v·ªõi OpenCV (thu·∫≠t to√°n kh√°c)"""
    
    try:
        print("üî¨ Upscale v·ªõi OpenCV...")
        
        # ƒê·ªçc ·∫£nh v·ªõi OpenCV
        img = cv2.imread(image_path)
        if img is None:
            print("‚ùå Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh v·ªõi OpenCV")
            return None
        
        height, width = img.shape[:2]
        print(f"üìê K√≠ch th∆∞·ªõc g·ªëc: {width}x{height}")
        
        # T√≠nh t·ª∑ l·ªá scale
        target_width = 1920
        target_height = 1080
        
        scale_x = target_width / width
        scale_y = target_height / height
        scale = min(scale_x, scale_y)
        
        new_width = int(width * scale)
        new_height = int(height * scale)
        
        print(f"üìè Scale: {scale:.2f}x -> {new_width}x{new_height}")
        
        # Upscale v·ªõi INTER_CUBIC (ch·∫•t l∆∞·ª£ng cao)
        img_resized = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)
        
        # T·∫°o canvas 1920x1080
        canvas = np.zeros((target_height, target_width, 3), dtype=np.uint8)
        
        # Center ·∫£nh
        x = (target_width - new_width) // 2
        y = (target_height - new_height) // 2
        
        canvas[y:y+new_height, x:x+new_width] = img_resized
        
        # √Åp d·ª•ng sharpening filter
        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
        canvas = cv2.filter2D(canvas, -1, kernel)
        
        # L∆∞u ·∫£nh
        cv2.imwrite(output_path, canvas, [cv2.IMWRITE_JPEG_QUALITY, 95])
        print(f"‚úÖ OpenCV upscale ƒë√£ l∆∞u: {output_path}")
        
        return output_path
        
    except Exception as e:
        print(f"‚ùå L·ªói OpenCV: {e}")
        return None


def upscale_try_on_results(try_on_dir, upscale_dir="upscaled_results"):
    """Upscale t·∫•t c·∫£ ·∫£nh try-on results l√™n 1920x1080"""
    
    if not os.path.exists(try_on_dir):
        print(f"‚ùå Th∆∞ m·ª•c try-on kh√¥ng t·ªìn t·∫°i: {try_on_dir}")
        return
    
    # T·∫°o th∆∞ m·ª•c upscale
    if not os.path.exists(upscale_dir):
        os.makedirs(upscale_dir)
    
    print(f"\nüöÄ B·∫Øt ƒë·∫ßu upscale t·∫•t c·∫£ ·∫£nh t·ª´ {try_on_dir}...")
    
    # L·∫•y t·∫•t c·∫£ file ·∫£nh
    image_files = []
    for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
        image_files.extend(glob.glob(os.path.join(try_on_dir, ext)))
    
    if not image_files:
        print("‚ùå Kh√¥ng t√¨m th·∫•y ·∫£nh n√†o ƒë·ªÉ upscale")
        return
    
    print(f"üìÅ T√¨m th·∫•y {len(image_files)} ·∫£nh ƒë·ªÉ upscale")
    
    successful_upscales = 0
    
    for i, image_path in enumerate(image_files):
        print(f"\nüì∏ ƒêang x·ª≠ l√Ω ({i+1}/{len(image_files)}): {os.path.basename(image_path)}")
        
        # T√™n file output
        filename = os.path.basename(image_path)
        name, ext = os.path.splitext(filename)
        output_path = os.path.join(upscale_dir, f"{name}_1080p{ext}")
        
        # Upscale
        result = upscale_with_opencv(image_path, output_path)
        
        if result:
            successful_upscales += 1
            file_size = os.path.getsize(result) / (1024 * 1024)  # MB
            print(f"‚úÖ Ho√†n th√†nh: {os.path.basename(result)} ({file_size:.1f} MB)")
        else:
            print(f"‚ùå Th·∫•t b·∫°i: {filename}")
    
    print(f"\nüéâ HO√ÄN TH√ÄNH UPSCALE!")
    print(f"‚úÖ Th√†nh c√¥ng: {successful_upscales}/{len(image_files)} ·∫£nh")
    print(f"üìÅ K·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u trong: {upscale_dir}")


def cleanup_directories():
    """
    D·ªçn d·∫πp c√°c th∆∞ m·ª•c sau khi t·∫°o k·∫øt qu·∫£:
    - X√≥a t·∫•t c·∫£ th∆∞ m·ª•c trong result/ tr·ª´ try_on/
    - X√≥a t·∫•t c·∫£ n·ªôi dung trong data/test/
    """
    
    # ƒê∆∞·ªùng d·∫´n g·ªëc
    base_path = os.path.dirname(os.path.abspath(__file__))
    result_path = os.path.join(base_path, 'result')
    data_test_path = os.path.join(base_path, 'data', 'test')
    
    print("üßπ B·∫Øt ƒë·∫ßu d·ªçn d·∫πp th∆∞ m·ª•c...")
    
    # 1. D·ªçn d·∫πp th∆∞ m·ª•c result (gi·ªØ l·∫°i try_on)
    if os.path.exists(result_path):
        print(f"üìÅ D·ªçn d·∫πp th∆∞ m·ª•c: {result_path}")
        
        for item in os.listdir(result_path):
            item_path = os.path.join(result_path, item)
            
            # B·ªè qua th∆∞ m·ª•c try_on
            if item == 'try_on':
                print(f"Gi·ªØ l·∫°i: {item}")
                continue
            
            # X√≥a c√°c th∆∞ m·ª•c/file kh√°c
            if os.path.isdir(item_path):
                try:
                    shutil.rmtree(item_path)
                    print(f"ƒê√£ x√≥a th∆∞ m·ª•c: {item}")
                except Exception as e:
                    print(f"L·ªói khi x√≥a {item}: {e}")
            elif os.path.isfile(item_path):
                try:
                    os.remove(item_path)
                    print(f"ƒê√£ x√≥a file: {item}")
                except Exception as e:
                    print(f"L·ªói khi x√≥a {item}: {e}")
    
    # 2. D·ªçn d·∫πp th∆∞ m·ª•c data/test
    if os.path.exists(data_test_path):
        print(f"üìÅ D·ªçn d·∫πp th∆∞ m·ª•c: {data_test_path}")
        
        for item in os.listdir(data_test_path):
            item_path = os.path.join(data_test_path, item)
            
            if os.path.isdir(item_path):
                try:
                    shutil.rmtree(item_path)
                    print(f"ƒê√£ x√≥a th∆∞ m·ª•c: {item}")
                except Exception as e:
                    print(f"L·ªói khi x√≥a {item}: {e}")
            elif os.path.isfile(item_path):
                try:
                    os.remove(item_path)
                    print(f"ƒê√£ x√≥a file: {item}")
                except Exception as e:
                    print(f"L·ªói khi x√≥a {item}: {e}")
    
    print("Ho√†n th√†nh d·ªçn d·∫πp!")


def get_opt():
    parser = argparse.ArgumentParser()

    parser.add_argument("--name", default="GMM")
    parser.add_argument("--gpu_ids", default="")
    parser.add_argument('-j', '--workers', type=int, default=1)
    parser.add_argument('-b', '--batch-size', type=int, default=4)
    parser.add_argument("--dataroot", default="data")
    parser.add_argument("--datamode", default="test")
    parser.add_argument("--stage", default="BOTH")  # GMM, TOM ho·∫∑c BOTH
    parser.add_argument("--data_list", default="test_pairs.txt")
    parser.add_argument("--fine_width", type=int, default=192)
    parser.add_argument("--fine_height", type=int, default=256)
    parser.add_argument("--radius", type=int, default=5)
    parser.add_argument("--grid_size", type=int, default=5)

    parser.add_argument('--gen_data', action='store_true', help='Generate data before testing')
    parser.add_argument('--source_dir', type=str, default='./input', help='Directory with source images')
    parser.add_argument('--output_dir', type=str, default='./data/test', help='Output directory for processed data')

    parser.add_argument('--tensorboard_dir', type=str,
                        default='tensorboard', help='save tensorboard infos')
    
    parser.add_argument('--result_dir_gmm', type=str,
                        default='data/test', help='save GMM result infos')
    parser.add_argument('--result_dir_tom', type=str,
                        default='result', help='save result infos')
    
    parser.add_argument('--gmm_checkpoint', type=str, default='checkpoints/GMM/gmm_final.pth', help='GMM model checkpoint')
    parser.add_argument('--tom_checkpoint', type=str, default='checkpoints/TOM/tom_final.pth', help='TOM model checkpoint')

    parser.add_argument("--display_count", type=int, default=1)
    parser.add_argument("--shuffle", action='store_true',
                        help='shuffle input data')
    parser.add_argument("--use_cuda", action='store_true',
                        help='use cuda')

    opt = parser.parse_args()
    # X√°c ƒë·ªãnh thi·∫øt b·ªã (CPU ho·∫∑c GPU)
    opt.device = torch.device('cuda' if opt.gpu_ids and torch.cuda.is_available() and opt.use_cuda else 'cpu')
    print(f"S·ª≠ d·ª•ng thi·∫øt b·ªã: {opt.device}")
    return opt


def test_gmm(opt, test_loader, model, board):
    model = model.to(opt.device)
    model.eval()

    name = "GMM"
    save_dir = os.path.join(opt.result_dir_gmm)
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    warp_cloth_dir = os.path.join(save_dir, 'warp_cloth')
    if not os.path.exists(warp_cloth_dir):
        os.makedirs(warp_cloth_dir)
    warp_mask_dir = os.path.join(save_dir, 'warp_mask')
    if not os.path.exists(warp_mask_dir):
        os.makedirs(warp_mask_dir)
    result_dir1 = os.path.join(save_dir, 'result_dir')
    if not os.path.exists(result_dir1):
        os.makedirs(result_dir1)
    overlayed_TPS_dir = os.path.join(save_dir, 'overlayed_TPS')
    if not os.path.exists(overlayed_TPS_dir):
        os.makedirs(overlayed_TPS_dir)
    warped_grid_dir = os.path.join(save_dir, 'warped_grid')
    if not os.path.exists(warped_grid_dir):
        os.makedirs(warped_grid_dir)
    
    print("ƒêang x·ª≠ l√Ω GMM...")
    for step, inputs in enumerate(test_loader.data_loader):
        iter_start_time = time.time()

        c_names = inputs['c_name']
        im_names = inputs['im_name']
        im = inputs['image'].to(opt.device)
        im_pose = inputs['pose_image'].to(opt.device)
        im_h = inputs['head'].to(opt.device)
        shape = inputs['shape'].to(opt.device)
        agnostic = inputs['agnostic'].to(opt.device)
        c = inputs['cloth'].to(opt.device)
        cm = inputs['cloth_mask'].to(opt.device)
        im_c = inputs['parse_cloth'].to(opt.device)
        im_g = inputs['grid_image'].to(opt.device)
        shape_ori = inputs['shape_ori']  # original body shape without blurring

        grid, theta = model(agnostic, cm)
        warped_cloth = F.grid_sample(c, grid, padding_mode='border')
        warped_mask = F.grid_sample(cm, grid, padding_mode='zeros')
        warped_grid = F.grid_sample(im_g, grid, padding_mode='zeros')
        overlay = 0.7 * warped_cloth + 0.3 * im

        visuals = [[im_h, shape, im_pose],
                   [c, warped_cloth, im_c],
                   [warped_grid, (warped_cloth+im)*0.5, im]]

        save_images(warped_cloth, im_names, warp_cloth_dir)
        save_images(warped_mask * 2 - 1, im_names, warp_mask_dir)
        save_images(shape_ori.to(opt.device) * 0.2 + warped_cloth *
                    0.8, im_names, result_dir1)
        save_images(warped_grid, im_names, warped_grid_dir)
        save_images(overlay, im_names, overlayed_TPS_dir)

        if (step+1) % opt.display_count == 0:
            board_add_images(board, 'combine', visuals, step+1)
            t = time.time() - iter_start_time
            print('step: %8d, time: %.3f' % (step+1, t), flush=True)
    
    return save_dir


def test_tom(opt, test_loader, model, board, gmm_outputs_dir=None):
    model = model.to(opt.device)
    model.eval()

    name = "TOM"
    save_dir = os.path.join(opt.result_dir_tom)
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    try_on_dir = os.path.join(save_dir, 'try_on')
    if not os.path.exists(try_on_dir):
        os.makedirs(try_on_dir)
    p_rendered_dir = os.path.join(save_dir, 'p_rendered')
    if not os.path.exists(p_rendered_dir):
        os.makedirs(p_rendered_dir)
    m_composite_dir = os.path.join(save_dir, 'm_composite')
    if not os.path.exists(m_composite_dir):
        os.makedirs(m_composite_dir)
    im_pose_dir = os.path.join(save_dir, 'im_pose')
    if not os.path.exists(im_pose_dir):
        os.makedirs(im_pose_dir)
    shape_dir = os.path.join(save_dir, 'shape')
    if not os.path.exists(shape_dir):
        os.makedirs(shape_dir)
    im_h_dir = os.path.join(save_dir, 'im_h')
    if not os.path.exists(im_h_dir):
        os.makedirs(im_h_dir)  # for test data

    print('Dataset size: %05d!' % (len(test_loader.dataset)), flush=True)
    print("ƒêang x·ª≠ l√Ω TOM...")
    
    for step, inputs in enumerate(test_loader.data_loader):
        iter_start_time = time.time()

        im_names = inputs['im_name']
        im = inputs['image'].to(opt.device)
        im_pose = inputs['pose_image']
        im_h = inputs['head']
        shape = inputs['shape']

        agnostic = inputs['agnostic'].to(opt.device)
        c = inputs['cloth'].to(opt.device)
        cm = inputs['cloth_mask'].to(opt.device)
        
        # N·∫øu c√≥ protected mask, s·ª≠ d·ª•ng n√≥
        # protected_mask = None
        # if 'protected_mask' in inputs:
        #     protected_mask = inputs['protected_mask'].to(opt.device)

        outputs = model(torch.cat([agnostic, c, cm], 1))  # CP-VTON+
        p_rendered, m_composite = torch.split(outputs, 3, 1)
        p_rendered = F.tanh(p_rendered)
        m_composite = F.sigmoid(m_composite)
        
        # N·∫øu c√≥ protected mask, √°p d·ª•ng n√≥ ƒë·ªÉ tr√°nh √°o d√≠nh l√™n t√≥c v√† c·ªï
        # if protected_mask is not None:
        #     m_composite = m_composite * (1 - protected_mask)
            
        p_tryon = c * m_composite + p_rendered * (1 - m_composite)

        visuals = [[im_h, shape, im_pose],
                   [c, 2*cm-1, m_composite],
                   [p_rendered, p_tryon, im]]

        save_images(p_tryon, im_names, try_on_dir)
        save_images(im_h, im_names, im_h_dir)
        save_images(shape, im_names, shape_dir)
        save_images(im_pose, im_names, im_pose_dir)
        save_images(m_composite, im_names, m_composite_dir)
        save_images(p_rendered, im_names, p_rendered_dir)  # For test data

        if (step+1) % opt.display_count == 0:
            board_add_images(board, 'combine', visuals, step+1)
            t = time.time() - iter_start_time
            print('step: %8d, time: %.3f' % (step+1, t), flush=True)
    
    return try_on_dir


def main():
    opt = get_opt()
    print(opt)
    
    # T·∫°o th∆∞ m·ª•c tensorboard n·∫øu ch∆∞a c√≥
    if not os.path.exists(opt.tensorboard_dir):
        os.makedirs(opt.tensorboard_dir)

    # N·∫øu c·∫ßn, t·∫°o d·ªØ li·ªáu tr∆∞·ªõc khi ch·∫°y    
    if opt.gen_data:
        print("Generating data before testing...")
        try:
            process_and_prepare_data(opt.source_dir, opt.output_dir)
            print("Data generation completed.")
        except Exception as e:
            print(f"L·ªói khi t·∫°o d·ªØ li·ªáu: {str(e)}")
            import traceback
            traceback.print_exc()

    tom_output_dir = None
    
    # Ch·∫°y GMM
    if opt.stage == 'GMM' or opt.stage == 'BOTH':
        print("=== ƒêang kh·ªüi t·∫°o giai ƒëo·∫°n GMM ===")
        # C·∫•u h√¨nh cho GMM
        opt_gmm = argparse.Namespace(**vars(opt))
        opt_gmm.stage = 'GMM'
        
        # T·∫°o dataset cho GMM
        test_dataset_gmm = CPDataset(opt_gmm)
        test_loader_gmm = CPDataLoader(opt_gmm, test_dataset_gmm)
        
        # T·∫°o board cho GMM
        board_gmm = SummaryWriter(logdir=os.path.join(opt.tensorboard_dir, 'GMM'))
        
        # Load model GMM
        model_gmm = GMM(opt_gmm)
        load_checkpoint(model_gmm, opt.gmm_checkpoint)
        
        # Ch·∫°y GMM
        with torch.no_grad():
            gmm_output_dir = test_gmm(opt_gmm, test_loader_gmm, model_gmm, board_gmm)
        
        print(f"Ho√†n th√†nh GMM, k·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u v√†o {gmm_output_dir}")
        
        # ƒê√≥ng board GMM khi xong
        board_gmm.close()
    
    # Ch·∫°y TOM
    if opt.stage == 'TOM' or opt.stage == 'BOTH':
        print("\n=== ƒêang kh·ªüi t·∫°o giai ƒëo·∫°n TOM ===")
        # C·∫•u h√¨nh cho TOM
        opt_tom = argparse.Namespace(**vars(opt))
        opt_tom.stage = 'TOM'
        
        # T·∫°o dataset m·ªõi cho TOM - quan tr·ªçng v√¨ logic dataset kh√°c nhau cho GMM v√† TOM
        test_dataset_tom = CPDataset(opt_tom)
        test_loader_tom = CPDataLoader(opt_tom, test_dataset_tom)
        
        # T·∫°o board cho TOM
        board_tom = SummaryWriter(logdir=os.path.join(opt.tensorboard_dir, 'TOM'))
        
        # Load model TOM
        model_tom = UnetGenerator(26, 4, 6, ngf=64, norm_layer=nn.InstanceNorm2d)  # CP-VTON+
        load_checkpoint(model_tom, opt.tom_checkpoint)
        
        # Ch·∫°y TOM
        with torch.no_grad():
            tom_output_dir = test_tom(opt_tom, test_loader_tom, model_tom, board_tom)
        
        print(f"Ho√†n th√†nh TOM, k·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u v√†o {tom_output_dir}")
        
        # ƒê√≥ng board TOM khi xong
        board_tom.close()
    
    print('\nQu√° tr√¨nh x·ª≠ l√Ω ƒë√£ ho√†n t·∫•t!')
    
    # Upscale t·ª± ƒë·ªông sau khi ho√†n th√†nh TOM
    if tom_output_dir:
        print('\nüöÄ B·∫Øt ƒë·∫ßu upscale k·∫øt qu·∫£ l√™n 1920x1080...')
        try:
            upscale_dir = "upscaled_results"
            upscale_try_on_results(tom_output_dir, upscale_dir)
            print(f'‚úÖ Ho√†n th√†nh upscale! K·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u trong: {upscale_dir}')
        except Exception as e:
            print(f'‚ùå L·ªói khi upscale: {e}')
    
    # D·ªçn d·∫πp t·ª± ƒë·ªông sau khi ho√†n th√†nh
    try:
        print('\nüßπ ƒêang d·ªçn d·∫πp th∆∞ m·ª•c...')
        cleanup_directories()
    except Exception as e:
        print(f'\n‚ùå L·ªói khi d·ªçn d·∫πp: {e}')


if __name__ == "__main__":
    main()
